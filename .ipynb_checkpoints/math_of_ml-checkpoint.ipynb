{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _The Math of Machine Learning_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Hey!\n",
    "\n",
    "You must have heard of the term Machine Learning generating a lot of hype these days. \n",
    "\n",
    "What do you think it really is? \n",
    "\n",
    "What do you think you would do as an \"ML Expert\"?\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<center><img src='img/meme1.png'><figcaption>We sincerely hope not this...</figcaption></figure></center>\n",
    "\n",
    "Let us begin with a simple definition of a Machine Learning Algorithm -\n",
    "<br>\n",
    "<br>\n",
    "<center>A computer program is said to learn from experience $E$ with respect to some class of tasks $T$ <br>and performance measure $P$ if its performance at tasks in $T$, as measured by $P$, improves with experience $E$ </center>\n",
    "\n",
    "For example, consider a program written to play the game of chess. \n",
    "\n",
    "- What is the Experience ($E$)?\n",
    "- What is the Task ($T$)?\n",
    "- What is the Performance Improvement ($P$)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning 101\n",
    "\n",
    "Now, lets try to model a machine learning problem and figure out what steps we require.\n",
    "\n",
    "1. Consider this - You are supposed to decide whether the next Salman Khan film would earn over 200 crores.\n",
    "\n",
    "    - What is the __first thing__ that you would need to decide this? (Think of the previous example of the chess game) {Data}\n",
    "\n",
    "    - After you get the movies, what is the __first__ thing that you will check? (PS - Ground Truth) {Whether the movie income is given}\n",
    "\n",
    "2. Suppose now, you're given a set of posters of various films and you need to differentiate one poster from others on the basis of the actor starring in them, but you do not know the number or the identity of the actors.\n",
    "\n",
    "    - How would you then go about the problem? {Unsupervised Learning - No labelling}\n",
    "    \n",
    "    \n",
    "You can see some examples from here - https://archive.ics.uci.edu/ml/datasets.html\n",
    "    \n",
    "    \n",
    "In Machine Learning, whatever data that you are given is separated is divided into two parts.\n",
    "- Training Data\n",
    "- Test Data\n",
    "\n",
    "The division is mostly 60% - 40% or 70% - 30% of the whole data, depending upon various cases. \n",
    "<br>\n",
    "\n",
    "\n",
    "<center><img src='img/flowchart.png'><figcaption>A concise idea you might have got so far</figcaption></figure></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets jump into the Math\n",
    "\n",
    "\n",
    "\n",
    "<center><img src='img/math.png'><figcaption>So much to study :/</figcaption></figure></center>\n",
    "\n",
    "It is surprising to see that there are many Machine Developers today who don't know the importance of mathematics in Machine Learning. At the other end of the spectrum, it is also suprprising to see people generating hype over the fact that the mathematics of ML is actually very tough. \n",
    "\n",
    "Here we will deal with a few basic algorithms of Machine Learning ans show you how with a little beyond high-school mathematics, you can apply them to problems on your own!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Remember the problem of predicting the total grossings of the next Salman Khan movie? \n",
    "\n",
    "Try and think about the following questions:\n",
    "\n",
    "- What are the factors that you would have to consider while predicting the grossings of a movie?\n",
    "- Once you have decided this, think, what would be your hypothesis?\n",
    "- Mathematically, could you form a relation between all these parameters and the price?\n",
    "\n",
    "What you just did up there was solve a problem using __Linear Regression__!\n",
    "\n",
    "#### Linear Regression: understanding graphically\n",
    "\n",
    "<br>\n",
    "\n",
    "<center><figure><img src='img/collage.jpg' height=500 weight=500></figure></center>\n",
    "\n",
    "\n",
    "#### Linear Regression: the math under the hood\n",
    "\n",
    "Let us take a mathematical look at this algorithm. For starters, as we discussed above we could use a linear relation to show the relationship between our __target values__ or the _net grossings_ and our selected __parameters__ or the _critic's ratings_. \n",
    "\n",
    "Let us define our hypothesis function as follows:\n",
    "\n",
    "$$ h_\\theta(x) = \\theta_0 + \\theta_1x $$\n",
    "<br>\n",
    "where $\\theta_0 = 1$ __(why?)__ and $x$ is the critics' rating. $h_\\theta(x)$ represents your hypothesis function that you will be using to predict the grossings of the next movie.\n",
    "\n",
    "- Q. But how do you judge if your predictions are right?\n",
    "    - Enter the __Cost function__ !\n",
    "    \n",
    "Never heard of it before? No worries!\n",
    "\n",
    "Imagine your computer charges you for the extent of the mistake that you make. The cost of each mistake is calculated by what is known as our old trusty ___cost function___!\n",
    "\n",
    "Here is an example cost function:\n",
    "\n",
    "$$ J(\\theta) = h_\\theta(x) - y $$\n",
    "\n",
    "This is the crux of any machine learning algorithm. You define a cost function and try to minimise it's value so as to achieve a local minima.\n",
    "\n",
    "Here we have to try and minimise the value of the cose function. So suppose you have $m$ samples in your trainng set, your cost function would be:\n",
    "\n",
    "$$ J(\\theta) = \\sum_{i=0}^{m} (h_\\theta(x_i) - y_i) $$\n",
    "\n",
    "- __Can you spot something wrong with this cost function?__\n",
    "   \n",
    "   - __HINT__: Try to figure out the value of the cost function of the data points plotted here:\n",
    "   <br>\n",
    "   <center><figure><img src='img/here.jpg' width=500 height=500></figure></center>\n",
    "   <br> \n",
    "- __Can you think of a better cost function?__\n",
    "\n",
    "Mostly, staticticians use the following cost function while employing linear regression to solve problems:\n",
    "\n",
    "$$ J(\\theta) =  \\frac{1}{2m}\\sum_{i=0}^{m} (h_\\theta(x_i) - y_i)^2 $$\n",
    "\n",
    "- What are the advantages of this cost function?\n",
    "- Why is the $\\frac{1}{2m}$ factor there in the formula?\n",
    "\n",
    "Now, once you know with the help of the __cost function__ about the correctness of your hypothesis, how do you update it to achieve a minimum cost?\n",
    "\n",
    "__HINT__: Notice that the cost function is a function of $\\theta$ and not $x$ or $y$. Does it give you an idea?\n",
    "\n",
    "To arrive at the corrent hypothesis, we use the method of __Gradient Descent__.\n",
    "\n",
    "Let us go back to the hypothesis that we had selected:\n",
    "\n",
    "$$ h_\\theta(x) = \\theta_0 + \\theta_1x $$\n",
    "\n",
    "Here, $\\theta_0=1$, hence we would use $ h_\\theta(x) = 1 + \\theta_1x $\n",
    "\n",
    "- Q. Now that we have decided __what__ to optimise, how do we go about it?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import play as p\n",
    "p.play()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now that you know what __Gradient Descent__ is in theory, how do we mathematically model it?\n",
    "\n",
    "- Clearly, we need to update our parameters for the cose function to decrease, but how do we achieve minima?\n",
    "\n",
    "   - __HINT__: How did you calculate local maxima in high school?\n",
    "\n",
    "To understand this, imagine another cost function for a hypothesis with a single parameter, i.e., $H_\\theta(x) = \\theta_0x$. \n",
    "\n",
    "- What will be the required cost function?\n",
    "- What would the graph of the cost function look like? Sketch it!\n",
    "- How would you decide how to update the parameter $\\theta_0$?\n",
    "\n",
    "    __HINTS__:\n",
    "\n",
    "    - What does a derivative signify?\n",
    "    - How can the derivative of the cost function help you in updation?\n",
    "    - Would you need to 'scale' the steps you need to take for the local minima? \n",
    "\n",
    "\n",
    "```\n",
    "repeat until convergence {\n",
    "    \n",
    "    calculate_cost_function()\n",
    "    \n",
    "    for all parameters {\n",
    "    \n",
    "        update parameter towards optima\n",
    "    \n",
    "    }\n",
    "\n",
    "}\n",
    "```\n",
    "\n",
    "- Q. What would be the problems you face in this approach?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#CODE FOR LINEAR REGRESSION GOES HERE\n",
    "\n",
    "import random as rd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sympy import *\n",
    "\n",
    "def get_update(x, y, weight_0,weight_1, m, alpha):\n",
    "    update_0 = 0\n",
    "    update_1 = 0\n",
    "    #print( round((1 / m),10))\n",
    "    for i in range(m-1):\n",
    "        update_0 +=  alpha * (1/m) * (y[i] - ((weight_1 * x[i]) + weight_0)) * (-1)\n",
    "        update_1 +=  alpha * (1/m) * (y[i] - ((weight_1 * x[i]) + weight_0)) * (-1) * weight_1\n",
    "        \n",
    "    #update = np.sum(alpha * round((1 / m),3) * np.round((( weight_0 + (weight_1 * x) - y ) * weight_1),2))\n",
    "    #print(update, \" update\")\n",
    "    \n",
    "    return update_0, update_1\n",
    "\n",
    "\n",
    "fh = open(\"data/data_lr.csv\",\"r\")\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for line in fh:\n",
    "    try:\n",
    "        tokens = line.split(\",\")\n",
    "        x.append(float(tokens[0].strip()))\n",
    "        y.append(float(tokens[1].strip()))\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "x = x[1:]\n",
    "y = y[1:]\n",
    "\n",
    "plt.plot(x,y,'o')\n",
    "    \n",
    "num_samples = len(x)\n",
    "\n",
    "param_0 = 1\n",
    "\n",
    "param_1 = 3\n",
    "\n",
    "params = [param_0, param_1]\n",
    "\n",
    "num_iters = 1000\n",
    "\n",
    "alpha = 0.0001\n",
    "\n",
    "for i in range(num_iters):\n",
    "\n",
    "    update_0, update_1 = get_update(x[1:], y[1:], param_0, param_1, num_samples, alpha)\n",
    "    tmp = param_1\n",
    "    param_0 = param_0 - update_0\n",
    "    param_1 = param_1 - update_1\n",
    "    #print(tmp,\"-\",(update),\"=\", param_1)\n",
    "    \n",
    "    \n",
    "print(param_0, param_1)\n",
    "\n",
    "h = list(map((lambda t: param_0 + (t * param_1)), x))\n",
    "\n",
    "plt.plot(x, h)\n",
    "plt.suptitle('Linear regression by gradient descent')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that you're given a set of movies, of different eras. You are allowed to see the movies and judge which ones are blockbuster hits and which ones are flops.\n",
    "\n",
    "- What criterias would you consider while judging the movies?\n",
    "- Any guesses what these 'criterias' are called?\n",
    "- Devise an algorithm to do so!\n",
    "- Wait. Is this information enough?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "As the name suggests, classification is a supervised learning approach in which the computer program learns from the data input given to it and then uses this learning to classify new observation. In the above example, 'Blockbuster hits' and 'flops' are called __classes.__\n",
    "\n",
    "This data set may simply be bi-class (like identifying whether the person is male or female or that the mail is spam or non-spam) or it may be multi-class too. Some examples of classification problems are: speech recognition, handwriting recognition, bio metric identification, document classification etc.\n",
    "\n",
    "We extract _'features'_ from our data and sync it with a class value (_'ground truth' or the label_) on our training data.\n",
    "Using the analysis that we get from the training data, we classify our test data into the classes\n",
    "\n",
    "- What exactly is the __analysis__ above? (Hint - Its Math!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Theorem\n",
    "\n",
    "Remember that theorem from the 12th standard math textbook that we thought was toooo easy? Its back.\n",
    "\n",
    "Given a hypothesis $H$ and evidence $E$, Bayes' theorem states that the relationship between the probability of the hypothesis before getting the evidence $P(H)$ and the probability of the hypothesis after getting the evidence $P(H \\space | \\space E)$ is:\n",
    "\n",
    "\n",
    "$$\n",
    "P(H \\space | \\space E) = \\frac{P(E \\space | \\space H)}{P(E)}P(H)\n",
    "$$\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier\n",
    "\n",
    "Now that you know the Bayes Theorem and what a classifier is, we come to a learning algorithm called the Naive Bayes Classifier.\n",
    "\n",
    "Consider this table:\n",
    "\n",
    "\n",
    "<center><img src='img/nbc_table1.png'><figcaption>Golf Playing Dataset</figcaption></figure></center>\n",
    "\n",
    "- What are the features present in this data?\n",
    "- What are the classes?\n",
    "\n",
    "Now, we need to use the Bayes Theorem to find the probability of the each class for each set of features.\n",
    "\n",
    "Here is the formula that we will use. \n",
    "\n",
    "$$ P(y \\space | \\space x_1 \\cap x_2 \\cap x_3 \\cap \\ldots \\cap x_n) = \\frac{P(x_1 \\cap x_2 \\cap x_3 \\cap \\ldots \\cap x_n \\space | \\space y)}{P(x_1 \\cap x_2 \\cap x_3 \\cap \\ldots \\cap x_n)}P(y)  $$\n",
    "\n",
    "- Can you make sense out of this formula? How did we arrive at this?\n",
    "- Devise an algorithm using the above formula\n",
    "- Are you stuck somewhere?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason this Bayes Classifier is called Naive because of the __2 assumptions__ that are made, which are very important!\n",
    "\n",
    "These assumption is that each feature makes an __independent__ and __equal__ contribution to the outcome.\n",
    "\n",
    "- How does this affect our formula above? \n",
    "\n",
    "By taking into account these 2 assumtions, we arrive at the new improved version of the Naive Bayes Classifer equation:\n",
    "\n",
    "$$ P(y \\space | \\space x_1 \\cap x_2 \\cap x_3 \\cap \\ldots \\cap x_n) = \\frac{P(x_1 \\space | \\space y)P(x_2 \\space | \\space y)P(x_3 \\space | \\space y) \\ldots P(x_n \\space | \\space y)}{P(x_1)P(x_2)P(x_3) \\ldots P(x_n)}P(y) $$\n",
    "\n",
    "Now the next question is how do we decide the values of the probabilities of the features? Here is a table that should help you:\n",
    "\n",
    "<center><img src='img/nbc_table2.png'><figcaption>Probabilities of the features</figcaption></figure></center>\n",
    "\n",
    "\n",
    "- How did we calculate all these values?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DIY\n",
    "\n",
    "The test problem we will use in this tutorial is the Pima Indians Diabetes problem.\n",
    "\n",
    "This problem is comprised of 768 observations of medical details for Pima indians patents. The records describe instantaneous measurements taken from the patient such as their age, the number of times pregnant and blood workup. All patients are women aged 21 or older. All attributes are numeric, and their units vary from attribute to attribute.\n",
    "\n",
    "Each record has a class value that indicates whether the patient suffered an onset of diabetes within 5 years of when the measurements were taken (1) or not (0).\n",
    "\n",
    "This is a standard dataset that has been studied a lot in machine learning literature. A good prediction accuracy is 70%-76%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Code goes here \n",
    "\n",
    "\"\"\"\n",
    "First you would need to go through the description/meta-data\n",
    "related to the dataset. It is present in the following file: \n",
    "\n",
    "    ./data/pima-indians-diabetes.names\n",
    "    \n",
    "Try going through the file and figure out how the dataset has\n",
    "been structured, what are the features, etc.\n",
    "\n",
    "The dataset is in the following file:\n",
    "\n",
    "    ./data/pima-indians-diabetes.csv\n",
    "\n",
    "Next, open the file in python as we did in the previous exa-\n",
    "ple and try playing around with it. Try to figure out how to \n",
    "extract the data from the file and how to store it.\n",
    "\n",
    "(HINT: Think of the data strucures in Python)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# code for printing the dataset goes here\n",
    "\n",
    "fh = open(\"./data/pima-indians-diabetes.csv\")\n",
    "data=[]\n",
    "for line in fh:\n",
    "    temp = line.split(\",\")\n",
    "    temp = list( map ( ( lambda x: float( x.strip() ) ),temp ) )\n",
    "    data.append(temp)\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "537\n",
      "231\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Now try and divide the data that we have into training and testing samples. \n",
    "Usual training/testing ratios are 60/40, 70/30 or 80/20\n",
    "Divide the dataset now into a ratio of 70/30\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "train_data = []  #This is the training data\n",
    "test_data = [] #This is the test data\n",
    "num_data = len(data)\n",
    "ratio = 0.7\n",
    "\n",
    "train_data = data[:int(num_data*ratio)]\n",
    "test_data = data[int(num_data*ratio):]\n",
    "\n",
    "num_tr_data = len(train_data)\n",
    "num_te_data = len(test_data)\n",
    "\n",
    "print(num_tr_data)\n",
    "print(num_te_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6480446927374302 0.35195530726256985\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Now we need to find the probabilities of each class. Since we just have 2 here,\n",
    "you need to find P(0) and P(1).\n",
    "\"\"\"\n",
    "\n",
    "num_0 = 0\n",
    "num_1 = 0\n",
    "\n",
    "for i in train_data:\n",
    "    if i[-1] == 0:\n",
    "        num_0+=1\n",
    "    else:\n",
    "        num_1+=1\n",
    "\n",
    "p_0 = num_0/num_tr_data\n",
    "p_1 = num_1/num_tr_data\n",
    "\n",
    "print(p_0,p_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now that we have our class probabilities, let us sort our training set according to the class values.\n",
    "This means that you need to segregate all the lines depending on the class they belong to.\n",
    "\n",
    "Fill the separate_class() function appropriately\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def separate_class(train_data):\n",
    "    separated_data = {}\n",
    "    for i in train_data:\n",
    "        if i[-1] not in separated_data:\n",
    "            separated_data[i[-1]] = []\n",
    "        separated_data[i[-1]].append(i)\n",
    "    return separated_data\n",
    "\n",
    "print(separate_class(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "After we have separated each data entry according to its class, let us find the probability of each attribute\n",
    "with respect to each class, i.e, P(x_i|y)\n",
    "\n",
    "How do you suggest we do that? (Think about distributions!)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Distribution\n",
    "\n",
    "In Gaussian Naive Bayes, continuous values associated with each feature are assumed to be distributed according to a Gaussian distribution. A Gaussian distribution is also called Normal distribution. When plotted, it gives a __bell shaped curve__ which is \n",
    "_symmetric_ about the _mean_ of the feature values. \n",
    "\n",
    "<center><img src='img/gd_nbc.png'><figcaption>Conditional Probability with Gaussian Distribution</figcaption></figure></center>\n",
    "\n",
    "Here, $ x_i $ is the $i^{th}$ attribute value, $ \\mu_y $ is the mean of that attribute when class $y$ is present, and $ \\sigma_y $ is the\n",
    "standard deviation of the attribute when class $y$ is present.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0 1.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Let us now quickly find out the mean and standard deviation. \n",
    "Fill the mean() and stddev() according to the given function definitions\n",
    "'numbers' is an array that is an input for the function\n",
    "\"\"\"\n",
    "import math\n",
    "\n",
    "def mean(numbers):\n",
    "    return sum(numbers)/float(len(numbers))\n",
    "\n",
    "def stddev(numbers):\n",
    "    avg = mean(numbers)\n",
    "    variance = sum([pow(x-avg,2) for x in numbers])/float(len(numbers)-1)\n",
    "    return math.sqrt(variance)\n",
    "\n",
    "print(mean([1,2,3]),stddev([1,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attribute summaries: [[2.0, 1.0], [21.0, 1.0]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Now just for our convenience, let us accumulate a statistic for all attributes for each class. \n",
    "We would call this 'summarizing' our data.\n",
    "\n",
    "We have already done this for you. You would have to print the output and deduce what it means.\n",
    "\n",
    "Now, let us combine the summarize() and separate_class() method in order to summarize each attribute\n",
    "according to the class.\n",
    "\n",
    "Complete summarize_class() according to the function definition.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def summarize(dataset):\n",
    "    summaries=[]\n",
    "    for attribute in range(len(dataset[0])):\n",
    "        temp = []\n",
    "        for row in dataset:\n",
    "            temp.append(row[attribute])\n",
    "        summaries.append([mean(temp),stddev(temp)])\n",
    "    del summaries[-1]\n",
    "    return summaries\n",
    "\n",
    "def summarize_class(dataset):\n",
    "    separated = separate_class(dataset)\n",
    "    summaries = {}\n",
    "    for classValue, instances in separated.items():\n",
    "        summaries[classValue] = summarize(instances)\n",
    "    return summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Next we code out a function for the gaussian distribution formula as mentioned above.\n",
    "Fill out the function calculate_probability() according to the function definition provided\n",
    "\"\"\"\n",
    "\n",
    "def calculate_probability(x, mean, stdev):\n",
    "\texponent = math.exp(-(math.pow(x-mean,2)/(2*math.pow(stdev,2))))\n",
    "\treturn (1 / (math.sqrt(2*math.pi) * stdev)) * exponent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def class_probability(summaries, input_vector):\n",
    "    probability_by_class={}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
